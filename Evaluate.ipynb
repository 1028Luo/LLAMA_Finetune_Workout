{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzbBrUkLDrqonih9l9bVX9"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install"
      ],
      "metadata": {
        "id": "EFDNQqFfMAW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from evaluate import load as load_metric\n",
        "import torch\n"
      ],
      "metadata": {
        "id": "oOFuItN2L-kJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qEsn7jiL3ZX"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Settings\n",
        "teacher_model_id = \"your-username/teacher-model-name\"\n",
        "student_model_id = \"your-username/student-model-name\"\n",
        "dataset_id = \"your-username/your-dataset-id\"\n",
        "test_size = 0.2\n",
        "max_input_length = 512\n",
        "max_gen_length = 128\n",
        "\n",
        "# Load original dataset (train only)\n",
        "full_dataset = load_dataset(dataset_id, split=\"train\")\n",
        "\n",
        "# Split into train/test\n",
        "split_dataset = full_dataset.train_test_split(test_size=test_size, seed=42)\n",
        "test_dataset = split_dataset[\"test\"]\n",
        "\n",
        "# Load models and tokenizer\n",
        "teacher_tokenizer = AutoTokenizer.from_pretrained(teacher_model_id)\n",
        "student_tokenizer = AutoTokenizer.from_pretrained(student_model_id)\n",
        "\n",
        "teacher_model = AutoModelForCausalLM.from_pretrained(teacher_model_id, torch_dtype=torch.float16).cuda()\n",
        "student_model = AutoModelForCausalLM.from_pretrained(student_model_id, torch_dtype=torch.float16).cuda()\n",
        "\n",
        "teacher_pipeline = pipeline(\"text-generation\", model=teacher_model, tokenizer=teacher_tokenizer, device=0)\n",
        "student_pipeline = pipeline(\"text-generation\", model=student_model, tokenizer=student_tokenizer, device=0)\n",
        "\n",
        "# Load evaluation metrics\n",
        "f1_metric = load_metric(\"f1\")\n",
        "bleu_metric = load_metric(\"bleu\")\n",
        "exact_match_metric = load_metric(\"exact_match\")\n",
        "\n",
        "# Format prompt for your task\n",
        "def format_prompt(example):\n",
        "    return f\"Question: {example['question']}\\nAnswer:\"\n",
        "\n",
        "# Generate answers\n",
        "def generate_answers(model_pipeline, inputs):\n",
        "    outputs = model_pipeline(inputs, max_new_tokens=max_gen_length, do_sample=False)\n",
        "    return [o[\"generated_text\"].replace(i, \"\").strip() for o, i in zip(outputs, inputs)]\n",
        "\n",
        "# Get prompts and references\n",
        "prompts = [format_prompt(x) for x in test_dataset]\n",
        "references = test_dataset[\"answer\"]\n",
        "bleu_refs = [[ref] for ref in references]\n",
        "\n",
        "# Generate predictions\n",
        "print(\"Generating with teacher...\")\n",
        "teacher_preds = generate_answers(teacher_pipeline, prompts)\n",
        "\n",
        "print(\"Generating with student...\")\n",
        "student_preds = generate_answers(student_pipeline, prompts)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(preds, refs):\n",
        "    f1 = f1_metric.compute(predictions=preds, references=refs)[\"f1\"]\n",
        "    bleu = bleu_metric.compute(predictions=preds, references=bleu_refs)[\"bleu\"]\n",
        "    em = exact_match_metric.compute(predictions=preds, references=refs)[\"exact_match\"]\n",
        "    return {\"f1\": f1, \"bleu\": bleu, \"exact_match\": em}\n",
        "\n",
        "# Evaluate both models\n",
        "teacher_scores = evaluate(teacher_preds, references)\n",
        "student_scores = evaluate(student_preds, references)\n",
        "\n",
        "# Print results\n",
        "print(\"\\n--- Evaluation Results ---\")\n",
        "print(\"Teacher Model:\")\n",
        "print(teacher_scores)\n",
        "\n",
        "print(\"Student Model:\")\n",
        "print(student_scores)\n",
        "\n"
      ]
    }
  ]
}