{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNQhcYiqJ06QaTLqy3bWxhr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1028Luo/LLM-Domain-Specific-Assistant/blob/main/finetune_LLAMA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "nJXyzcfvy1oR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# install dependencies and import\n",
        "\n",
        "# llama index\n",
        "!pip install llama-index-core\n",
        "!pip install llama-index-llms-openai\n",
        "!pip install llama-index-llms-replicate\n",
        "!pip install llama-index-embeddings-huggingface\n",
        "\n",
        "!pip install \"transformers>=4.45.1\"\n",
        "!pip install huggingface_hub\n",
        "!pip install transformers\n",
        "!pip install bitsandbytes\n",
        "!pip install peft\n",
        "!pip install accelerate\n",
        "\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
        "# load fine-tuned model from hub\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch\n",
        "import bitsandbytes\n",
        "from huggingface_hub import login\n"
      ],
      "metadata": {
        "id": "6rDNh0oSxhln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if this colab runtime has gpu, set device automatically\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'  # Use GPU\n",
        "    print(\"GPU is available, using GPU\")\n",
        "else:\n",
        "    device = 'cpu'  # Use CPU if no GPU is found\n",
        "    print(\"Not connected to a GPU, using CPU\")\n",
        "\n",
        "print(f\"Using device: {device}\")\n"
      ],
      "metadata": {
        "id": "I_zfxOflEY2L",
        "outputId": "23ed722b-5b41-4a89-903b-0ef2bc9bd393",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not connected to a GPU, using CPU\n",
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Log in to Hugging Face and load model\n",
        "\n",
        "# login\n",
        "from google.colab import userdata\n",
        "my_hugging_face_token = userdata.get('huggingface_token')\n",
        "login(token=my_hugging_face_token)\n",
        "\n",
        "# load model and tokenizer\n",
        "model_name = \"meta-llama/Llama-3.2-1B\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n"
      ],
      "metadata": {
        "id": "zRmfntM7CtoX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B4tFNh8KUWeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# quantization\n",
        "\n",
        "# print model size in GB\n",
        "model_size_bytes = model.get_memory_footprint()\n",
        "model_size_gb = model_size_bytes / (1024 ** 3)  # 1 GB = 1024^3 bytes\n",
        "print(f\"Model size: {model_size_gb:,.2f} GB\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhKwCagJC5NC",
        "outputId": "2d4e46b0-2fdd-447f-f949-9da7f2c665ed"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model size: 4.60 GB\n"
          ]
        }
      ]
    }
  ]
}